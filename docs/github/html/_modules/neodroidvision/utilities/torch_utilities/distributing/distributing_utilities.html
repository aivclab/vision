<!DOCTYPE html>

<html lang='en'>
  <head>
    <meta charset='utf-8'/>
    <meta name='viewport' content='width=device-width, initial-scale=1.0'/>
    <title>neodroidvision.utilities.torch_utilities.distributing.distributing_utilities &#8212; neodroidvision 0.2.9 documentation</title>
    <link rel='stylesheet' type='text/css' href='../../../../../_static/pygments.css'/>
    <link rel='stylesheet' type='text/css' href='../../../../../_static/alabaster.css'/>
    <link rel='stylesheet' type='text/css' href='../../../../../_static/graphviz.css'/>
    <script data-url_root='../../../../../' id='documentation_options' src='../../../../../_static/documentation_options.js'></script>
    <script src='../../../../../_static/jquery.js'></script>
    <script src='../../../../../_static/underscore.js'></script>
    <script src='../../../../../_static/_sphinx_javascript_frameworks_compat.js'></script>
    <script src='../../../../../_static/doctools.js'></script>
    <link rel='canonical' href='aivclab.github.io/neodroidvision/_modules/neodroidvision/utilities/torch_utilities/distributing/distributing_utilities.html'/>
    <link rel='index' title='Index' href='../../../../../genindex.html'/>
    <link rel='search' title='Search' href='../../../../../search.html'/>

    <link rel='stylesheet' href='../../../../../_static/custom.css' type='text/css'/>


    <meta name='viewport' content='width=device-width, initial-scale=0.9, maximum-scale=0.9'/>

  </head>
  <body>


    <div class='document'>
      <div class='documentwrapper'>
        <div class='bodywrapper'>


          <div class='body' role='main'>

            <h1>Source code for neodroidvision.utilities.torch_utilities.distributing.distributing_utilities</h1>
            <div class='highlight'><pre>
<span></span><span class='ch'>#!/usr/bin/env python3</span>
<span class='c1'># -*- coding: utf-8 -*-</span>

<span class='n'>__author__</span> <span class='o'>=</span> <span class='s2'>&quot;Christian Heider Nielsen&quot;</span>
<span class='vm'>__doc__</span> <span class='o'>=</span> <span class='sa'>r</span><span class='s2'>&quot;&quot;&quot;</span>

<span class='s2'>           Created on 01/03/2020</span>
<span class='s2'>           &quot;&quot;&quot;</span>

<span class='kn'>import</span> <span class='nn'>logging</span>
<span class='kn'>import</span> <span class='nn'>os</span>
<span class='kn'>import</span> <span class='nn'>sys</span>
<span class='kn'>from</span> <span class='nn'>pathlib</span> <span class='kn'>import</span> <span class='n'>Path</span>
<span class='kn'>from</span> <span class='nn'>typing</span> <span class='kn'>import</span> <span class='n'>Any</span><span class='p'>,</span> <span class='n'>List</span>

<span class='kn'>import</span> <span class='nn'>torch</span>
<span class='kn'>import</span> <span class='nn'>torch.utils.data</span>
<span class='kn'>from</span> <span class='nn'>torch</span> <span class='kn'>import</span> <span class='n'>distributed</span>

<span class='kn'>from</span> <span class='nn'>neodroidvision.utilities.torch_utilities.distributing.serialisation</span> <span class='kn'>import</span> <span class='p'>(</span>
    <span class='n'>deserialise_byte_tensor</span><span class='p'>,</span>
    <span class='n'>to_byte_tensor</span><span class='p'>,</span>
<span class='p'>)</span>

<span class='n'>__all__</span> <span class='o'>=</span> <span class='p'>[</span>
    <span class='s2'>&quot;all_gather_cuda&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;reduce_dict&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;setup_for_distributed&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;is_distribution_ready&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;is_main_process&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;init_distributed_mode&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;save_on_master&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;global_distribution_rank&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;global_world_size&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;set_benchmark_device_dist&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;synchronise_torch_barrier&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;setup_distributed_logger&quot;</span><span class='p'>,</span>
<span class='p'>]</span>


<div class='viewcode-block' id='is_distribution_ready'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.is_distribution_ready.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.is_distribution_ready'>[docs]</a><span class='k'>def</span> <span class='nf'>is_distribution_ready</span><span class='p'>()</span> <span class='o'>-&gt;</span> <span class='nb'>bool</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>

<span class='sd'>    :return:&quot;&quot;&quot;</span>
    <span class='k'>if</span> <span class='ow'>not</span> <span class='n'>distributed</span><span class='o'>.</span><span class='n'>is_available</span><span class='p'>():</span>
        <span class='k'>return</span> <span class='kc'>False</span>
    <span class='k'>if</span> <span class='ow'>not</span> <span class='n'>distributed</span><span class='o'>.</span><span class='n'>is_initialized</span><span class='p'>():</span>
        <span class='k'>return</span> <span class='kc'>False</span>
    <span class='k'>return</span> <span class='kc'>True</span></div>


<div class='viewcode-block' id='global_world_size'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.global_world_size.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.global_world_size'>[docs]</a><span class='k'>def</span> <span class='nf'>global_world_size</span><span class='p'>()</span> <span class='o'>-&gt;</span> <span class='nb'>int</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>

<span class='sd'>    :return:&quot;&quot;&quot;</span>
    <span class='k'>if</span> <span class='ow'>not</span> <span class='n'>is_distribution_ready</span><span class='p'>():</span>
        <span class='k'>return</span> <span class='mi'>1</span>
    <span class='k'>return</span> <span class='n'>distributed</span><span class='o'>.</span><span class='n'>get_world_size</span><span class='p'>()</span></div>


<div class='viewcode-block' id='global_distribution_rank'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.global_distribution_rank.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.global_distribution_rank'>[docs]</a><span class='k'>def</span> <span class='nf'>global_distribution_rank</span><span class='p'>()</span> <span class='o'>-&gt;</span> <span class='nb'>int</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>

<span class='sd'>    :return:&quot;&quot;&quot;</span>
    <span class='k'>if</span> <span class='ow'>not</span> <span class='n'>is_distribution_ready</span><span class='p'>():</span>
        <span class='k'>return</span> <span class='mi'>0</span>
    <span class='k'>return</span> <span class='n'>distributed</span><span class='o'>.</span><span class='n'>get_rank</span><span class='p'>()</span></div>


<div class='viewcode-block' id='is_main_process'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.is_main_process.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.is_main_process'>[docs]</a><span class='k'>def</span> <span class='nf'>is_main_process</span><span class='p'>()</span> <span class='o'>-&gt;</span> <span class='nb'>bool</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>

<span class='sd'>    :return:&quot;&quot;&quot;</span>
    <span class='k'>return</span> <span class='n'>global_distribution_rank</span><span class='p'>()</span> <span class='o'>==</span> <span class='mi'>0</span></div>


<div class='viewcode-block' id='save_on_master'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.save_on_master.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.save_on_master'>[docs]</a><span class='k'>def</span> <span class='nf'>save_on_master</span><span class='p'>(</span><span class='o'>*</span><span class='n'>args</span><span class='p'>,</span> <span class='o'>**</span><span class='n'>kwargs</span><span class='p'>)</span> <span class='o'>-&gt;</span> <span class='kc'>None</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>

<span class='sd'>    :param args:</span>
<span class='sd'>    :param kwargs:</span>
<span class='sd'>    :return:&quot;&quot;&quot;</span>
    <span class='k'>if</span> <span class='n'>is_main_process</span><span class='p'>():</span>
        <span class='n'>torch</span><span class='o'>.</span><span class='n'>save</span><span class='p'>(</span><span class='o'>*</span><span class='n'>args</span><span class='p'>,</span> <span class='o'>**</span><span class='n'>kwargs</span><span class='p'>)</span></div>


<div class='viewcode-block' id='setup_for_distributed'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.setup_for_distributed.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.setup_for_distributed'>[docs]</a><span class='k'>def</span> <span class='nf'>setup_for_distributed</span><span class='p'>(</span><span class='n'>is_master</span><span class='p'>:</span> <span class='nb'>bool</span><span class='p'>)</span> <span class='o'>-&gt;</span> <span class='kc'>None</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>
<span class='sd'>    This function disables printing when not in master process&quot;&quot;&quot;</span>
    <span class='kn'>import</span> <span class='nn'>builtins</span> <span class='k'>as</span> <span class='nn'>__builtin__</span>

    <span class='n'>builtin_print</span> <span class='o'>=</span> <span class='n'>__builtin__</span><span class='o'>.</span><span class='n'>print</span>

    <span class='k'>def</span> <span class='nf'>print</span><span class='p'>(</span><span class='o'>*</span><span class='n'>args</span><span class='p'>,</span> <span class='o'>**</span><span class='n'>kwargs</span><span class='p'>):</span>
        <span class='sd'>&quot;&quot;&quot;</span>

<span class='sd'>        Args:</span>
<span class='sd'>          *args:</span>
<span class='sd'>          **kwargs:</span>
<span class='sd'>        &quot;&quot;&quot;</span>
        <span class='n'>force</span> <span class='o'>=</span> <span class='n'>kwargs</span><span class='o'>.</span><span class='n'>pop</span><span class='p'>(</span><span class='s2'>&quot;force&quot;</span><span class='p'>,</span> <span class='kc'>False</span><span class='p'>)</span>
        <span class='k'>if</span> <span class='n'>is_master</span> <span class='ow'>or</span> <span class='n'>force</span><span class='p'>:</span>
            <span class='n'>builtin_print</span><span class='p'>(</span><span class='o'>*</span><span class='n'>args</span><span class='p'>,</span> <span class='o'>**</span><span class='n'>kwargs</span><span class='p'>)</span>

    <span class='n'>__builtin__</span><span class='o'>.</span><span class='n'>print</span> <span class='o'>=</span> <span class='nb'>print</span></div>


<div class='viewcode-block' id='all_gather_cuda'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.all_gather_cuda.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.all_gather_cuda'>[docs]</a><span class='k'>def</span> <span class='nf'>all_gather_cuda</span><span class='p'>(</span><span class='n'>data</span><span class='p'>:</span> <span class='n'>Any</span><span class='p'>)</span> <span class='o'>-&gt;</span> <span class='n'>List</span><span class='p'>[</span><span class='nb'>bytes</span><span class='p'>]:</span>
    <span class='sd'>&quot;&quot;&quot;</span>
<span class='sd'>    Run all_gather on arbitrary picklable data (not necessarily tensors)</span>
<span class='sd'>    Args:</span>
<span class='sd'>    data: any picklable object</span>
<span class='sd'>    Returns:</span>
<span class='sd'>    list[data]: list of data gathered from each rank&quot;&quot;&quot;</span>
    <span class='n'>world_size</span> <span class='o'>=</span> <span class='n'>global_world_size</span><span class='p'>()</span>
    <span class='k'>if</span> <span class='n'>world_size</span> <span class='o'>==</span> <span class='mi'>1</span><span class='p'>:</span>
        <span class='k'>return</span> <span class='p'>[</span><span class='n'>data</span><span class='p'>]</span>

    <span class='n'>tensor</span> <span class='o'>=</span> <span class='n'>to_byte_tensor</span><span class='p'>(</span><span class='n'>data</span><span class='p'>,</span> <span class='n'>device</span><span class='o'>=</span><span class='s2'>&quot;cuda&quot;</span><span class='p'>)</span>

    <span class='c1'># obtain Tensor size of each rank</span>
    <span class='n'>local_size</span> <span class='o'>=</span> <span class='n'>torch</span><span class='o'>.</span><span class='n'>tensor</span><span class='p'>([</span><span class='n'>tensor</span><span class='o'>.</span><span class='n'>numel</span><span class='p'>()],</span> <span class='n'>device</span><span class='o'>=</span><span class='s2'>&quot;cuda&quot;</span><span class='p'>)</span>
    <span class='n'>size_list</span> <span class='o'>=</span> <span class='p'>[</span><span class='n'>torch</span><span class='o'>.</span><span class='n'>tensor</span><span class='p'>([</span><span class='mi'>0</span><span class='p'>],</span> <span class='n'>device</span><span class='o'>=</span><span class='s2'>&quot;cuda&quot;</span><span class='p'>)</span> <span class='k'>for</span> <span class='n'>_</span> <span class='ow'>in</span> <span class='nb'>range</span><span class='p'>(</span><span class='n'>world_size</span><span class='p'>)]</span>
    <span class='n'>distributed</span><span class='o'>.</span><span class='n'>all_gather</span><span class='p'>(</span><span class='n'>size_list</span><span class='p'>,</span> <span class='n'>local_size</span><span class='p'>)</span>
    <span class='n'>size_list</span> <span class='o'>=</span> <span class='p'>[</span><span class='nb'>int</span><span class='p'>(</span><span class='n'>size</span><span class='o'>.</span><span class='n'>item</span><span class='p'>())</span> <span class='k'>for</span> <span class='n'>size</span> <span class='ow'>in</span> <span class='n'>size_list</span><span class='p'>]</span>
    <span class='n'>max_size</span> <span class='o'>=</span> <span class='nb'>max</span><span class='p'>(</span><span class='n'>size_list</span><span class='p'>)</span>

    <span class='c1'># receiving Tensor from all ranks</span>
    <span class='c1'># we pad the tensor because torch all_gather does not support</span>
    <span class='c1'># gathering tensors of different shapes</span>
    <span class='n'>tensor_list</span> <span class='o'>=</span> <span class='p'>[]</span>
    <span class='k'>for</span> <span class='n'>_</span> <span class='ow'>in</span> <span class='n'>size_list</span><span class='p'>:</span>
        <span class='n'>tensor_list</span><span class='o'>.</span><span class='n'>append</span><span class='p'>(</span><span class='n'>torch</span><span class='o'>.</span><span class='n'>empty</span><span class='p'>((</span><span class='n'>max_size</span><span class='p'>,),</span> <span class='n'>dtype</span><span class='o'>=</span><span class='n'>torch</span><span class='o'>.</span><span class='n'>uint8</span><span class='p'>,</span> <span class='n'>device</span><span class='o'>=</span><span class='s2'>&quot;cuda&quot;</span><span class='p'>))</span>
    <span class='k'>if</span> <span class='n'>local_size</span> <span class='o'>!=</span> <span class='n'>max_size</span><span class='p'>:</span>
        <span class='n'>padding</span> <span class='o'>=</span> <span class='n'>torch</span><span class='o'>.</span><span class='n'>empty</span><span class='p'>(</span>
            <span class='n'>size</span><span class='o'>=</span><span class='p'>(</span><span class='n'>max_size</span> <span class='o'>-</span> <span class='n'>local_size</span><span class='p'>,),</span> <span class='n'>dtype</span><span class='o'>=</span><span class='n'>torch</span><span class='o'>.</span><span class='n'>uint8</span><span class='p'>,</span> <span class='n'>device</span><span class='o'>=</span><span class='s2'>&quot;cuda&quot;</span>
        <span class='p'>)</span>
        <span class='n'>tensor</span> <span class='o'>=</span> <span class='n'>torch</span><span class='o'>.</span><span class='n'>cat</span><span class='p'>((</span><span class='n'>tensor</span><span class='p'>,</span> <span class='n'>padding</span><span class='p'>),</span> <span class='n'>dim</span><span class='o'>=</span><span class='mi'>0</span><span class='p'>)</span>
    <span class='n'>distributed</span><span class='o'>.</span><span class='n'>all_gather</span><span class='p'>(</span><span class='n'>tensor_list</span><span class='p'>,</span> <span class='n'>tensor</span><span class='p'>)</span>

    <span class='k'>return</span> <span class='n'>deserialise_byte_tensor</span><span class='p'>(</span><span class='n'>size_list</span><span class='p'>,</span> <span class='n'>tensor_list</span><span class='p'>)</span></div>


<div class='viewcode-block' id='reduce_dict'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.reduce_dict.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.reduce_dict'>[docs]</a><span class='k'>def</span> <span class='nf'>reduce_dict</span><span class='p'>(</span><span class='n'>input_dict</span><span class='p'>:</span> <span class='nb'>dict</span><span class='p'>,</span> <span class='n'>average</span><span class='p'>:</span> <span class='nb'>bool</span> <span class='o'>=</span> <span class='kc'>True</span><span class='p'>)</span> <span class='o'>-&gt;</span> <span class='nb'>dict</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>
<span class='sd'>    Args:</span>
<span class='sd'>    input_dict (dict): all the values will be reduced</span>
<span class='sd'>    average (bool): whether to do average or sum</span>
<span class='sd'>    Reduce the values in the dictionary from all processes so that all processes</span>
<span class='sd'>    have the averaged results. Returns a dict with the same fields as</span>
<span class='sd'>    input_dict, after reduction.&quot;&quot;&quot;</span>
    <span class='n'>world_size</span> <span class='o'>=</span> <span class='n'>global_world_size</span><span class='p'>()</span>
    <span class='k'>if</span> <span class='n'>world_size</span> <span class='o'>&lt;</span> <span class='mi'>2</span><span class='p'>:</span>
        <span class='k'>return</span> <span class='n'>input_dict</span>
    <span class='k'>with</span> <span class='n'>torch</span><span class='o'>.</span><span class='n'>no_grad</span><span class='p'>():</span>
        <span class='n'>names</span> <span class='o'>=</span> <span class='p'>[]</span>
        <span class='n'>values</span> <span class='o'>=</span> <span class='p'>[]</span>
        <span class='c1'># sort the keys so that they are consistent across processes</span>
        <span class='k'>for</span> <span class='n'>k</span> <span class='ow'>in</span> <span class='nb'>sorted</span><span class='p'>(</span><span class='n'>input_dict</span><span class='o'>.</span><span class='n'>keys</span><span class='p'>()):</span>
            <span class='n'>names</span><span class='o'>.</span><span class='n'>append</span><span class='p'>(</span><span class='n'>k</span><span class='p'>)</span>
            <span class='n'>values</span><span class='o'>.</span><span class='n'>append</span><span class='p'>(</span><span class='n'>input_dict</span><span class='p'>[</span><span class='n'>k</span><span class='p'>])</span>
        <span class='n'>values</span> <span class='o'>=</span> <span class='n'>torch</span><span class='o'>.</span><span class='n'>stack</span><span class='p'>(</span><span class='n'>values</span><span class='p'>,</span> <span class='n'>dim</span><span class='o'>=</span><span class='mi'>0</span><span class='p'>)</span>
        <span class='n'>distributed</span><span class='o'>.</span><span class='n'>all_reduce</span><span class='p'>(</span><span class='n'>values</span><span class='p'>)</span>
        <span class='k'>if</span> <span class='n'>average</span><span class='p'>:</span>
            <span class='n'>values</span> <span class='o'>/=</span> <span class='n'>world_size</span>
        <span class='n'>reduced_dict</span> <span class='o'>=</span> <span class='p'>{</span><span class='n'>k</span><span class='p'>:</span> <span class='n'>v</span> <span class='k'>for</span> <span class='n'>k</span><span class='p'>,</span> <span class='n'>v</span> <span class='ow'>in</span> <span class='nb'>zip</span><span class='p'>(</span><span class='n'>names</span><span class='p'>,</span> <span class='n'>values</span><span class='p'>)}</span>
    <span class='k'>return</span> <span class='n'>reduced_dict</span></div>


<div class='viewcode-block' id='init_distributed_mode'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.init_distributed_mode.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.init_distributed_mode'>[docs]</a><span class='k'>def</span> <span class='nf'>init_distributed_mode</span><span class='p'>(</span><span class='n'>args</span><span class='p'>:</span> <span class='n'>Any</span><span class='p'>)</span> <span class='o'>-&gt;</span> <span class='kc'>None</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>

<span class='sd'>    Args:</span>
<span class='sd'>      args:</span>

<span class='sd'>    Returns:</span>

<span class='sd'>    &quot;&quot;&quot;</span>
    <span class='k'>if</span> <span class='s2'>&quot;RANK&quot;</span> <span class='ow'>in</span> <span class='n'>os</span><span class='o'>.</span><span class='n'>environ</span> <span class='ow'>and</span> <span class='s2'>&quot;WORLD_SIZE&quot;</span> <span class='ow'>in</span> <span class='n'>os</span><span class='o'>.</span><span class='n'>environ</span><span class='p'>:</span>
        <span class='n'>args</span><span class='o'>.</span><span class='n'>rank</span> <span class='o'>=</span> <span class='nb'>int</span><span class='p'>(</span><span class='n'>os</span><span class='o'>.</span><span class='n'>environ</span><span class='p'>[</span><span class='s2'>&quot;RANK&quot;</span><span class='p'>])</span>
        <span class='n'>args</span><span class='o'>.</span><span class='n'>world_size</span> <span class='o'>=</span> <span class='nb'>int</span><span class='p'>(</span><span class='n'>os</span><span class='o'>.</span><span class='n'>environ</span><span class='p'>[</span><span class='s2'>&quot;WORLD_SIZE&quot;</span><span class='p'>])</span>
        <span class='n'>args</span><span class='o'>.</span><span class='n'>gpu</span> <span class='o'>=</span> <span class='nb'>int</span><span class='p'>(</span><span class='n'>os</span><span class='o'>.</span><span class='n'>environ</span><span class='p'>[</span><span class='s2'>&quot;LOCAL_RANK&quot;</span><span class='p'>])</span>
    <span class='k'>elif</span> <span class='s2'>&quot;SLURM_PROCID&quot;</span> <span class='ow'>in</span> <span class='n'>os</span><span class='o'>.</span><span class='n'>environ</span><span class='p'>:</span>
        <span class='n'>args</span><span class='o'>.</span><span class='n'>rank</span> <span class='o'>=</span> <span class='nb'>int</span><span class='p'>(</span><span class='n'>os</span><span class='o'>.</span><span class='n'>environ</span><span class='p'>[</span><span class='s2'>&quot;SLURM_PROCID&quot;</span><span class='p'>])</span>
        <span class='n'>args</span><span class='o'>.</span><span class='n'>gpu</span> <span class='o'>=</span> <span class='n'>args</span><span class='o'>.</span><span class='n'>rank</span> <span class='o'>%</span> <span class='n'>torch</span><span class='o'>.</span><span class='n'>cuda</span><span class='o'>.</span><span class='n'>device_count</span><span class='p'>()</span>
    <span class='k'>else</span><span class='p'>:</span>
        <span class='nb'>print</span><span class='p'>(</span><span class='s2'>&quot;Not using distributed mode&quot;</span><span class='p'>)</span>
        <span class='n'>args</span><span class='o'>.</span><span class='n'>distributed</span> <span class='o'>=</span> <span class='kc'>False</span>
        <span class='k'>return</span>

    <span class='n'>args</span><span class='o'>.</span><span class='n'>distributed</span> <span class='o'>=</span> <span class='kc'>True</span>
    <span class='n'>torch</span><span class='o'>.</span><span class='n'>cuda</span><span class='o'>.</span><span class='n'>set_device</span><span class='p'>(</span><span class='n'>args</span><span class='o'>.</span><span class='n'>gpu</span><span class='p'>)</span>
    <span class='n'>args</span><span class='o'>.</span><span class='n'>dist_backend</span> <span class='o'>=</span> <span class='s2'>&quot;nccl&quot;</span>

    <span class='nb'>print</span><span class='p'>(</span><span class='sa'>f</span><span class='s2'>&quot;| distributed init (rank </span><span class='si'>{</span><span class='n'>args</span><span class='o'>.</span><span class='n'>rank</span><span class='si'>}</span><span class='s2'>): </span><span class='si'>{</span><span class='n'>args</span><span class='o'>.</span><span class='n'>dist_url</span><span class='si'>}</span><span class='s2'>&quot;</span><span class='p'>,</span> <span class='n'>flush</span><span class='o'>=</span><span class='kc'>True</span><span class='p'>)</span>
    <span class='n'>torch</span><span class='o'>.</span><span class='n'>distributed</span><span class='o'>.</span><span class='n'>init_process_group</span><span class='p'>(</span>
        <span class='n'>backend</span><span class='o'>=</span><span class='n'>args</span><span class='o'>.</span><span class='n'>dist_backend</span><span class='p'>,</span>
        <span class='n'>init_method</span><span class='o'>=</span><span class='n'>args</span><span class='o'>.</span><span class='n'>dist_url</span><span class='p'>,</span>
        <span class='n'>world_size</span><span class='o'>=</span><span class='n'>args</span><span class='o'>.</span><span class='n'>world_size</span><span class='p'>,</span>
        <span class='n'>rank</span><span class='o'>=</span><span class='n'>args</span><span class='o'>.</span><span class='n'>rank</span><span class='p'>,</span>
    <span class='p'>)</span>
    <span class='n'>torch</span><span class='o'>.</span><span class='n'>distributed</span><span class='o'>.</span><span class='n'>barrier</span><span class='p'>()</span>
    <span class='n'>setup_for_distributed</span><span class='p'>(</span><span class='n'>args</span><span class='o'>.</span><span class='n'>rank</span> <span class='o'>==</span> <span class='mi'>0</span><span class='p'>)</span></div>


<div class='viewcode-block' id='synchronise_torch_barrier'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.synchronise_torch_barrier.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.synchronise_torch_barrier'>[docs]</a><span class='k'>def</span> <span class='nf'>synchronise_torch_barrier</span><span class='p'>()</span> <span class='o'>-&gt;</span> <span class='kc'>None</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>
<span class='sd'>    Helper function to synchronize (barrier) among all processes when</span>
<span class='sd'>    using distributed training&quot;&quot;&quot;</span>
    <span class='k'>if</span> <span class='ow'>not</span> <span class='n'>is_distribution_ready</span><span class='p'>():</span>
        <span class='k'>return</span>
    <span class='n'>world_size</span> <span class='o'>=</span> <span class='n'>distributed</span><span class='o'>.</span><span class='n'>get_world_size</span><span class='p'>()</span>
    <span class='k'>if</span> <span class='n'>world_size</span> <span class='o'>==</span> <span class='mi'>1</span><span class='p'>:</span>
        <span class='k'>return</span>
    <span class='n'>distributed</span><span class='o'>.</span><span class='n'>barrier</span><span class='p'>()</span></div>


<div class='viewcode-block' id='setup_distributed_logger'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.setup_distributed_logger.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.setup_distributed_logger'>[docs]</a><span class='k'>def</span> <span class='nf'>setup_distributed_logger</span><span class='p'>(</span>
    <span class='n'>name</span><span class='p'>:</span> <span class='nb'>str</span><span class='p'>,</span> <span class='n'>distributed_rank</span><span class='p'>:</span> <span class='nb'>int</span><span class='p'>,</span> <span class='n'>save_dir</span><span class='p'>:</span> <span class='n'>Path</span> <span class='o'>=</span> <span class='kc'>None</span>
<span class='p'>)</span> <span class='o'>-&gt;</span> <span class='n'>logging</span><span class='o'>.</span><span class='n'>Logger</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>

<span class='sd'>    :param name:</span>
<span class='sd'>    :param distributed_rank:</span>
<span class='sd'>    :param save_dir:</span>
<span class='sd'>    :return:&quot;&quot;&quot;</span>
    <span class='n'>logger</span> <span class='o'>=</span> <span class='n'>logging</span><span class='o'>.</span><span class='n'>getLogger</span><span class='p'>(</span><span class='n'>name</span><span class='p'>)</span>
    <span class='n'>logger</span><span class='o'>.</span><span class='n'>setLevel</span><span class='p'>(</span><span class='n'>logging</span><span class='o'>.</span><span class='n'>DEBUG</span><span class='p'>)</span>
    <span class='c1'># don&#39;t log results for the non-master process</span>
    <span class='k'>if</span> <span class='n'>distributed_rank</span> <span class='o'>&gt;</span> <span class='mi'>0</span><span class='p'>:</span>
        <span class='k'>return</span> <span class='n'>logger</span>
    <span class='n'>stream_handler</span> <span class='o'>=</span> <span class='n'>logging</span><span class='o'>.</span><span class='n'>StreamHandler</span><span class='p'>(</span><span class='n'>stream</span><span class='o'>=</span><span class='n'>sys</span><span class='o'>.</span><span class='n'>stdout</span><span class='p'>)</span>
    <span class='n'>stream_handler</span><span class='o'>.</span><span class='n'>setLevel</span><span class='p'>(</span><span class='n'>logging</span><span class='o'>.</span><span class='n'>DEBUG</span><span class='p'>)</span>
    <span class='n'>formatter</span> <span class='o'>=</span> <span class='n'>logging</span><span class='o'>.</span><span class='n'>Formatter</span><span class='p'>(</span><span class='s2'>&quot;</span><span class='si'>%(asctime)s</span><span class='s2'> </span><span class='si'>%(name)s</span><span class='s2'> </span><span class='si'>%(levelname)s</span><span class='s2'>: </span><span class='si'>%(message)s</span><span class='s2'>&quot;</span><span class='p'>)</span>
    <span class='n'>stream_handler</span><span class='o'>.</span><span class='n'>setFormatter</span><span class='p'>(</span><span class='n'>formatter</span><span class='p'>)</span>
    <span class='n'>logger</span><span class='o'>.</span><span class='n'>addHandler</span><span class='p'>(</span><span class='n'>stream_handler</span><span class='p'>)</span>
    <span class='k'>if</span> <span class='n'>save_dir</span><span class='p'>:</span>
        <span class='n'>fh</span> <span class='o'>=</span> <span class='n'>logging</span><span class='o'>.</span><span class='n'>FileHandler</span><span class='p'>(</span><span class='nb'>str</span><span class='p'>(</span><span class='n'>save_dir</span> <span class='o'>/</span> <span class='s2'>&quot;log.txt&quot;</span><span class='p'>))</span>
        <span class='n'>fh</span><span class='o'>.</span><span class='n'>setLevel</span><span class='p'>(</span><span class='n'>logging</span><span class='o'>.</span><span class='n'>DEBUG</span><span class='p'>)</span>
        <span class='n'>fh</span><span class='o'>.</span><span class='n'>setFormatter</span><span class='p'>(</span><span class='n'>formatter</span><span class='p'>)</span>
        <span class='n'>logger</span><span class='o'>.</span><span class='n'>addHandler</span><span class='p'>(</span><span class='n'>fh</span><span class='p'>)</span>
    <span class='k'>return</span> <span class='n'>logger</span></div>


<div class='viewcode-block' id='set_benchmark_device_dist'><a class='viewcode-back' href='../../../../../generated/neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.set_benchmark_device_dist.html#neodroidvision.utilities.torch_utilities.distributing.distributing_utilities.set_benchmark_device_dist'>[docs]</a><span class='k'>def</span> <span class='nf'>set_benchmark_device_dist</span><span class='p'>(</span><span class='n'>distributed</span><span class='p'>:</span> <span class='nb'>bool</span><span class='p'>,</span> <span class='n'>local_rank</span><span class='p'>:</span> <span class='nb'>int</span><span class='p'>)</span> <span class='o'>-&gt;</span> <span class='kc'>None</span><span class='p'>:</span>
    <span class='sd'>&quot;&quot;&quot;</span>

<span class='sd'>    :param distributed:</span>
<span class='sd'>    :param local_rank:</span>
<span class='sd'>    :return:&quot;&quot;&quot;</span>
    <span class='k'>if</span> <span class='n'>torch</span><span class='o'>.</span><span class='n'>cuda</span><span class='o'>.</span><span class='n'>is_available</span><span class='p'>():</span>
        <span class='c1'># This flag allows you to enable the inbuilt cudnn auto-tuner to</span>
        <span class='c1'># find the best algorithm to use for your hardware.</span>
        <span class='n'>torch</span><span class='o'>.</span><span class='n'>backends</span><span class='o'>.</span><span class='n'>cudnn</span><span class='o'>.</span><span class='n'>benchmark</span> <span class='o'>=</span> <span class='kc'>True</span>

    <span class='k'>if</span> <span class='n'>distributed</span><span class='p'>:</span>
        <span class='n'>torch</span><span class='o'>.</span><span class='n'>cuda</span><span class='o'>.</span><span class='n'>set_device</span><span class='p'>(</span><span class='n'>local_rank</span><span class='p'>)</span>
        <span class='n'>torch</span><span class='o'>.</span><span class='n'>distributed</span><span class='o'>.</span><span class='n'>init_process_group</span><span class='p'>(</span><span class='n'>backend</span><span class='o'>=</span><span class='s2'>&quot;nccl&quot;</span><span class='p'>,</span> <span class='n'>init_method</span><span class='o'>=</span><span class='s2'>&quot;env://&quot;</span><span class='p'>)</span>
        <span class='n'>synchronise_torch_barrier</span><span class='p'>()</span></div>
</pre>
            </div>

          </div>

        </div>
      </div>
      <div class='sphinxsidebar' role='navigation' aria-label='main navigation'>
        <div class='sphinxsidebarwrapper'>
          <h1 class='logo'><a href='../../../../../index.html'>neodroidvision</a></h1>


          <h3>Navigation</h3>
          <ul>
            <li class='toctree-l1'><a class='reference internal' href='../../../../../generated/neodroidvision.html'>neodroidvision</a></li>
          </ul>
          <p class='caption' role='heading'><span class='caption-text'>Notes</span></p>
          <ul>
            <li class='toctree-l1'><a class='reference internal' href='../../../../../getting_started.html'>Getting Started</a></li>
          </ul>

          <div class='relations'>
            <h3>Related Topics</h3>
            <ul>
              <li><a href='../../../../../index.html'>Documentation overview</a>
                <ul>
                  <li><a href='../../../../index.html'>Module code</a>
                    <ul>
                      <li><a href='../../../../neodroidvision.html'>neodroidvision</a>
                        <ul>
                        </ul>
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </div>
          <div id='searchbox' style='display: none' role='search'>
            <h3 id='searchlabel'>Quick search</h3>
            <div class='searchformwrapper'>
              <form class='search' action='../../../../../search.html' method='get'>
                <input type='text' name='q' aria-labelledby='searchlabel' autocomplete='off' autocorrect='off' autocapitalize='off' spellcheck='false'/>
                <input type='submit' value='Go'/>
              </form>
            </div>
          </div>
          <script>document.getElementById('searchbox').style.display = "block"</script>


        </div>
      </div>
      <div class='clearer'></div>
    </div>
    <div class='footer'>
      &copy;.

      |
      Powered by <a href='http://sphinx-doc.org/'>Sphinx 5.0.2</a>
      &amp; <a href='https://github.com/bitprophet/alabaster'>Alabaster 0.7.12</a>

    </div>


  </body>
</html>